{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lab Five: Wide and Deep Networks</h1>\n",
    "<b>By Michael Watts, Maya Muralidhar, Nora Potenti, and Adam Ashcraft </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.0 Preparation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.1 Business Understanding </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set for this lab is a collection of synthetic online transactions produced by The PaySim simulator and collected by the Norwegian University of Science and Technology. The simulation was created based on real financial data for a multinational company. It is categorized by the type of the transaction (i.e. a payment, transfer, etc.), the original balance before and after the transaction of the source account, the balance before and after the transaction of the destination account, and if the transaction was actually fraud. It is also marked by several other categories not useful for this use case. As more and more transactions shift from the physical space to the digital space, it becomes more important for financial institutions to be able to detect and deny fraudulent charges. As the number of digital transactions increases, the amount of data to parse to determine the legitimacy of a transaction increases to the point where these companies could not afford humans to do the fraud detection. This is where our model would come in, as an efficient learning tool able to detect and mark fraud for these institutions. Romexsoft, a company that helps develop fraud detection models, boast a 98% fraud detection rate. For our model to be a success, it must detect at or above this rate. \n",
    "<hr>\n",
    "Kaggle link: https://www.kaggle.com/ntnu-testimon/paysim1/home  <br>\n",
    "Romexsoft: https://www.romexsoft.com/blog/credit-card-fraud-detection-in-banking/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2 Data Cleaning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import missingno as mn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6362620 entries, 0 to 6362619\n",
      "Data columns (total 11 columns):\n",
      "step              int64\n",
      "type              object\n",
      "amount            float64\n",
      "nameOrig          object\n",
      "oldbalanceOrg     float64\n",
      "newbalanceOrig    float64\n",
      "nameDest          object\n",
      "oldbalanceDest    float64\n",
      "newbalanceDest    float64\n",
      "isFraud           int64\n",
      "isFlaggedFraud    int64\n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 534.0+ MB\n"
     ]
    }
   ],
   "source": [
    "finData = pd.read_csv('data/PS_20174392719_1491204439457_log.csv') #load the data\n",
    "finData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by removing some of the columns we will not be using. After this, we will convert the oldbalanceOrg and newbalanceOrig into one column that reflects the change the original account balance, called Org_Account_Delta. If it is an increase in account balance, we will make it a 1. If it is a decrease we will make it a negative 1. No change will be a 0. We will do the same thing for oldbalanceDest and newbalanceDest in Dest_Account_Delta. Finally, we will encode the one-hot encode the 6 types: CASH-IN, CASH-OUT, NAN, DEBIT, PAYMENT and TRANSFER. If a transaction is fraudulent, in the isFraud column, it will be marked as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finData.drop(columns=['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], inplace=True)\n",
    "def calcChange1(change):\n",
    "    #change = row.newbalanceOrig - row.oldbalanceOrg\n",
    "    if(change > 0):\n",
    "        return 'positive'\n",
    "    if(change < 0):\n",
    "        return 'negative'\n",
    "    if(change == 0):\n",
    "        return 'zero'\n",
    "def calcChange2(change):\n",
    "    #change = row.newbalanceDest - row.oldbalanceDest\n",
    "    if(change > 0):\n",
    "        return 'positive'\n",
    "    if(change < 0):\n",
    "        return 'negative'\n",
    "    if(change == 0):\n",
    "        return 'zero'\n",
    "finData['Org_Account_Delta'] = finData['newbalanceOrig']-finData['oldbalanceOrg']\n",
    "finData['Dest_Account_Delta'] = finData['newbalanceDest']-finData['oldbalanceDest']\n",
    "finData['Org_Account_Delta'] = finData['Org_Account_Delta'].apply(lambda x: calcChange1(x))\n",
    "finData['Dest_Account_Delta'] = finData['Dest_Account_Delta'].apply(lambda x: calcChange2(x))\n",
    "finData.drop(columns=['newbalanceOrig', 'newbalanceDest'], inplace=True)\n",
    "finData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pickle our data for faster reterival. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(finData, open( 'pickledData/finData.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finData = pickle.load(open( 'pickledData/finData.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn.matrix(finData)\n",
    "print(mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have no missing data that needs to be imputed. We have our Y target, isFraud. We also have the type of transaction and the amount of each transaction. We have generated columns to show the change in each account with each transaction as well. Finally, we also know the account balance for the origin and destination accounts before each transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.3 Cross-Product Features </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will cross Dest_Account_Delta and type. This could reveal a correlation between the type of transaction used money being sent into a destination account. If scammers found a method to execute a certain kind of transaction on customer accounts, they would use the same transaction type over and over to extract money from a customer account into their own.  Next we will cross Org_Account_Delta and type. This will help us establish the same kind of correlation from the previous column cross. It will however also reveal to us possible correlations where scammers are directly drawing from customer accounts without using a secondary account. Logically scammers would try this method as much as possible, as it does not involve adding a secondary account with a paper trail that could possibly lead back to them. If these kinds of fraudulent transactions are common, this cross will reveal it. Finally, we will cross Dest_Account_Delta and Org_Account_Delta. This will help further learn the correlation between when money leaves a customer account and ends up in a new account. Untimely this will help further cement either the correlation between a high number of direct fraudulent withdraws, or fraudulent money transfers.     \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_product_sets= [['type', 'Dest_Account_Delta'], \n",
    "                     [ 'type', 'Org_Account_Delta'], \n",
    "                    ['Dest_Account_Delta', 'Org_Account_Delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.4 Evaluation Criteria </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our evaluation criteria, we will be focused on recall. Recall focuses on the amount of false negatives achieved. In this instance a false negative means marking a fraudulent transaction as a genuine one. A false positive means we have marked a genuine transaction as a fraudulent one. In a false positive scenario, the client may be slightly inconvenienced. He would have to call the bank and ensure the proper funds are released. In a false negative situation, the clientâ€™s money has been illegally transferred from his account and is most likely lost to him forever. The bank will have to spend time both reimbursing his account and filing the proper paperwork about the fraudulent attempt. The client will be unhappy the bank has not properly secured his money and the criminal has just successful conned the bank. In order to prevent what would be the worse case scenario for a mislabeled transaction, we will focus on keeping our recall and subsequently our false negative rate low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scorer = make_scorer(recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.5 Data Division <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data, we will use a stratified shuffle split. The stratification of the data ensures that each k-fold will have the same percentage of fraudulent data as the data set as a whole. This prevents the model from ever receiving and being trained off of a data set with no fraudulent transactions present. If this were the case, our network may simply detect everything as genuine and still have a good evaluation score. shuffling will prevent any one account from being disproportionally present in a fold. As this data was originally linear time data, it is possible one account would be making several hundreds of transactions sequentially. For instance, a company may be restocking all its inventory at once. To avoid this, we will shuffle the data. While 10 splits would have been the best choice, for the sake of computation, we will bound it with 5 folds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample\n",
    "sampledFinData = pd.concat([finData.loc[finData['isFraud']==0].sample(frac=.01), finData.loc[finData['isFraud']==1]])\n",
    "sampledFinData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we will divide out our X and Y data\n",
    "y = sampledFinData.isFraud\n",
    "finData.drop(columns=['isFraud'], inplace=True)\n",
    "X = sampledFinData\n",
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=1) \n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2> 2.0 Modeling </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1 Train/Test/Split to Divide Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.layers import Embedding, Flatten, Concatenate\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "X_train_ints = X_train.copy()\n",
    "X_test_ints = X_test.copy()\n",
    "print(X_train.shape)\n",
    "\n",
    "categorical_headers = ['type', 'Org_Account_Delta', 'Dest_Account_Delta']\n",
    "numeric_headers = ['amount', 'oldbalanceOrg', 'oldbalanceDest']\n",
    "\n",
    "# standard scale numeric columns\n",
    "for col in numeric_headers:\n",
    "    X_train[col] = X_train[col].astype(np.float)\n",
    "    X_test[col] = X_test[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    X_train[col] = ss.fit_transform(X_train[col].values.reshape(-1, 1))\n",
    "    X_test[col] = ss.transform(X_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "\n",
    "for col in ['type', 'Org_Account_Delta', 'Dest_Account_Delta']:\n",
    "    train_labels, train_levels = pd.factorize(X_train[col])\n",
    "    test_labels, test_levels = pd.factorize(X_test[col])\n",
    "    X_train_ints[col] = train_labels\n",
    "    X_test_ints[col] = test_labels\n",
    "    \n",
    "ohe = OneHotEncoder()    \n",
    "X_train_ohe = ohe.fit_transform(X_train_ints[['type', 'Org_Account_Delta', 'Dest_Account_Delta']].values)\n",
    "X_test_ohe = ohe.transform(X_test_ints[['type', 'Org_Account_Delta', 'Dest_Account_Delta']].values)\n",
    "\n",
    "X_train_num =  X_train[['amount', 'oldbalanceOrg', 'oldbalanceDest']].values\n",
    "X_test_num = X_test[['amount', 'oldbalanceOrg', 'oldbalanceDest']].values\n",
    "#X_train = X_train.values\n",
    "#X_test = X_test.values\n",
    "#y_train = y_train.values\n",
    "#y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# From Stack Overflow: \n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_ints(crossed_columns, X_train_t, X_test_t):\n",
    "    X_ints_train = [] # keep track of inputs for each branch\n",
    "    X_ints_test = []\n",
    "    X_train_ints = X_train_t.copy()\n",
    "    X_test_ints = X_test_t.copy()\n",
    "    enc = LabelEncoder()\n",
    "    for cols in crossed_columns:\n",
    "        X_crossed_train = X_train_t[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = X_test_t[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "    for col in ['type', 'Org_Account_Delta', 'Dest_Account_Delta']:\n",
    "        train_labels, train_levels = pd.factorize(X_train_t[col])\n",
    "        test_labels, test_levels = pd.factorize(X_test_t[col])\n",
    "        X_train_ints[col] = train_labels\n",
    "        X_test_ints[col] = test_labels\n",
    "    for col in categorical_headers:\n",
    "        X_ints_train.append( X_train_ints[col].values )\n",
    "        X_ints_test.append( X_test_ints[col].values )\n",
    "    return  X_ints_train, X_ints_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(crossed_columns, num_deep_layers, X_train, X_test):\n",
    "    X_ints_train = [] # keep track of inputs for each branch\n",
    "    X_ints_test = []# keep track of inputs for each branch\n",
    "    all_inputs = [] # this is what we will give to keras.Model inputs\n",
    "    all_branch_outputs = [] # this is where we will keep track of output of each branch\n",
    "    all_wide_branch_outputs = []\n",
    "    all_deep_branch_outputs = []\n",
    "    for cols in crossed_columns:\n",
    "        # encode crossed columns as ints for the embedding\n",
    "        enc = LabelEncoder()\n",
    "\n",
    "        # create crossed labels\n",
    "        X_crossed_train = X_train[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "        X_crossed_test = X_test[cols].apply(lambda x: '_'.join(x), axis=1)\n",
    "\n",
    "        enc.fit(np.hstack((X_crossed_train.values,  X_crossed_test.values)))\n",
    "        X_crossed_train = enc.transform(X_crossed_train)\n",
    "        X_crossed_test = enc.transform(X_crossed_test)\n",
    "        X_ints_train.append( X_crossed_train )\n",
    "        X_ints_test.append( X_crossed_test )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name = '_'.join(cols))\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, \n",
    "                      output_dim=int(np.sqrt(N)), \n",
    "                      input_length=1, name = '_'.join(cols)+'_embed')(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_wide_branch_outputs.append(x)\n",
    "    \n",
    "    # merge the branches together\n",
    "    wide_branch = concatenate(all_wide_branch_outputs, name='wide_concat')\n",
    "    wide_branch = Dense(units=1,activation='sigmoid',name='wide_combined')(wide_branch)\n",
    "\n",
    "    for col in categorical_headers:\n",
    "        # encode as ints for the embedding\n",
    "        X_ints_train.append( X_train_ints[col].values )\n",
    "        X_ints_test.append( X_test_ints[col].values )\n",
    "\n",
    "        # get the number of categories\n",
    "        N = max(X_ints_train[-1]+1) # same as the max(df_train[col])\n",
    "\n",
    "        # create embedding branch from the number of categories\n",
    "        inputs = Input(shape=(1,),dtype='int32', name=col)\n",
    "        all_inputs.append(inputs)\n",
    "        x = Embedding(input_dim=N, \n",
    "                      output_dim=int(np.sqrt(N)), \n",
    "                      input_length=1, name=col+'_embed')(inputs)\n",
    "        x = Flatten()(x)\n",
    "        all_deep_branch_outputs.append(x)\n",
    "    \n",
    "    # also get a dense branch of the numeric features\n",
    "    all_inputs.append(Input(shape=(X_train_num.shape[1],),\n",
    "                            sparse=False,\n",
    "                            name='numeric_data'))\n",
    "\n",
    "    x = Dense(units=20, activation='tanh',name='numeric_1')(all_inputs[-1])\n",
    "    all_deep_branch_outputs.append( x )\n",
    "    \n",
    "    #x = concatenate([xSparse, xDense], name='concat')\n",
    "    #all_deep_branch_outputs.append( x )\n",
    "\n",
    "    # merge the deep branches together\n",
    "    deep_branch = concatenate(all_deep_branch_outputs,name='concat_embeds')\n",
    "    if(num_deep_layers >= 1):\n",
    "        deep_branch = Dense(units=6,activation='tanh', name='deep1')(deep_branch)\n",
    "        print('Deep 1 created') # just to check\n",
    "    if(num_deep_layers > 1):\n",
    "        deep_branch = Dense(units=3,activation='tanh', name='deep2')(deep_branch)\n",
    "        print('Deep 2 created') # just to check\n",
    "    if(num_deep_layers > 2):\n",
    "        deep_branch = Dense(units=3,activation='tanh', name='deep3')(deep_branch)\n",
    "        print('Deep 3 created') # just to check    \n",
    "    \n",
    "    final_branch = concatenate([wide_branch, deep_branch],name='concat_deep_wide')\n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "    final_branch = Dense(units=1,activation='sigmoid',name='combined')(final_branch)\n",
    "    model = Model(inputs=all_inputs, outputs=final_branch)\n",
    "    model.compile(optimizer='sgd',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=[as_keras_metric(tf.metrics.recall), 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_product_sets_1 = cross_product_sets[:2]\n",
    "X_ints_train, X_ints_test = get_x_ints(crossed_product_sets_1, X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_1 = KerasClassifier(build_fn=create_model,\n",
    "                        crossed_columns=crossed_product_sets_1,\n",
    "                        num_deep_layers=2,\n",
    "                        X_train=X_train,\n",
    "                        X_test=X_test,\n",
    "                        epochs=5, \n",
    "                        batch_size=100,\n",
    "                        verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ints_train.append(X_train_num)\n",
    "X_ints_test.append(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_performance = network_1.fit(X_ints_train, y_train, validation_data = (X_ints_test, y_test))\n",
    "#results = cross_val_score(network_1, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(model_1_performance.history['recall'])\n",
    "plt.ylabel('Recall %')\n",
    "\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(model_1_performance.history['val_recall'])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(model_1_performance.history['loss'])\n",
    "plt.ylabel('MSE Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(model_1_performance.history['val_loss'])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2 Stratified Shuffle Split </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_set = []\n",
    "#X_test_set = []\n",
    "#y_train_set = []\n",
    "#y_test_set = []\n",
    "\n",
    "\n",
    "#for train_index, test_index in cv.split(X, y): \n",
    "#    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "#    y_train_set.append(y_train)\n",
    "#    y_test_set.append(y_test)\n",
    "\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# standard scale numerical headers    \n",
    "#for each in numeric_headers:\n",
    "#    X_train[each] = X_train[each].astype(np.float)\n",
    "#    X_test[each] = X_test[each].astype(np.float)\n",
    "    \n",
    "#    ss = StandardScaler()\n",
    "#    X_train[each] = ss.fit_transform(X_train[each].values.reshape(-1, 1))\n",
    "#    X_test[each] = ss.transform(X_test[each].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_product_sets_2 = cross_product_sets\n",
    "X_ints_train_2, X_ints_test_2 = get_x_ints(crossed_product_sets_2, X_train, X_test)\n",
    "X_ints_train_2.append(X_train_num)\n",
    "X_ints_test_2.append(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_2 = KerasClassifier(build_fn=create_model,\n",
    "                        crossed_columns=crossed_product_sets_2,\n",
    "                        num_deep_layers=3,\n",
    "                        X_train=X_train,\n",
    "                        X_test=X_test,\n",
    "                        epochs=5, \n",
    "                        batch_size=100,\n",
    "                        verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_performance = network_2.fit(X_ints_train_2, y_train, validation_data = (X_ints_test_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(model_2_performance.history['recall'])\n",
    "plt.ylabel('Recall %')\n",
    "\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(model_2_performance.history['val_recall'])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(model_2_performance.history['loss'])\n",
    "plt.ylabel('MSE Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(model_2_performance.history['val_loss'])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in cv.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_shuffle_train, X_shuffle_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_shuffle_train, y_shuffle_test = y[train_index], y[test_index]\n",
    "    for col in numeric_headers:\n",
    "        X_shuffle_train[col] = X_shuffle_train[col].astype(np.float)\n",
    "        X_shuffle_test[col] = X_shuffle_test[col].astype(np.float)\n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X_shuffle_train[col] = ss.fit_transform(X_shuffle_train[col].values.reshape(-1, 1))\n",
    "        X_shuffle_test[col] = ss.transform(X_shuffle_test[col].values.reshape(-1, 1))\n",
    "\n",
    "    X_shuffle_train_num =  X_shuffle_train[['amount', 'oldbalanceOrg', 'oldbalanceDest']].values\n",
    "    X_shuffle_test_num = X_shuffle_test[['amount', 'oldbalanceOrg', 'oldbalanceDest']].values\n",
    "    X_shuffle_ints_train, X_shuffle_ints_test = get_x_ints(crossed_product_sets_2, X_shuffle_train, X_shuffle_test)\n",
    "    X_shuffle_ints_train.append(X_shuffle_train_num)\n",
    "    print(X_shuffle_ints_train[1].shape)\n",
    "    X_shuffle_ints_test.append(X_shuffle_test_num)\n",
    "    shuffle_network = KerasClassifier(build_fn=create_model,\n",
    "                        crossed_columns=crossed_product_sets_2,\n",
    "                        num_deep_layers=3,\n",
    "                        X_train=X_shuffle_train,\n",
    "                        X_test=X_shuffle_test,\n",
    "                        epochs=5, \n",
    "                        batch_size=100,\n",
    "                        verbose=1,)\n",
    "    shuffle_network.fit(X_shuffle_ints_train, y_shuffle_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_ints_test[0]))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
