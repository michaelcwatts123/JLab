{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lab Three: Extending Logistic Regression</h1>\n",
    "<b>By Michael Watts, Maya Muralidhar, Nora Potenti, and Adam Ashcraft </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Business Understanding\n",
    "\n",
    "This dataset is from the Free Music Archive, a collection of legally available audio files.  The numerical data is made up of features extracted from a musical analysis python package called librosa that quantifies some of the characteristics of an mp3 and also includes statistics such as mean, skew, and kurtosis.  The dataset also includes unique codes for genres of music.  We will use these features to determine what the genre of a piece is given the librosa feature extraction.  This classification would prove useful for a music streaming application such as Spotify that would want to integrate new music into its platform quickly, especially if the defined genre in the audio file's metadata doesn't matches one of the genres defined in the application's database.  It would also help with the application's recommendation system; by broadly defining the main genre categories, users could receive recommendations that are audially similar.  For this use case, the model would be deployed to a production.To measure our success, we will compare our results to the International Society for Music Information Retrieval’s Music Information Retrieval Evaluation eXchange (MIREX) 2017 competition winning algorithm LPNKK1, which has an accuracy of approximately 77%. Our algorithm is considered a success if it can outperform this academic algorithm’s genre classification success rate.\n",
    "<hr>\n",
    "Data Set Source: https://github.com/mdeff/fma <br>\n",
    "MIREX Competition Results: http://www.music-ir.org/nema_out/mirex2017/results/act/mixed_report/summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>chroma_cens</th>\n",
       "      <th>chroma_cens.1</th>\n",
       "      <th>chroma_cens.2</th>\n",
       "      <th>chroma_cens.3</th>\n",
       "      <th>chroma_cens.4</th>\n",
       "      <th>chroma_cens.5</th>\n",
       "      <th>chroma_cens.6</th>\n",
       "      <th>chroma_cens.7</th>\n",
       "      <th>chroma_cens.8</th>\n",
       "      <th>...</th>\n",
       "      <th>tonnetz.39</th>\n",
       "      <th>tonnetz.40</th>\n",
       "      <th>tonnetz.41</th>\n",
       "      <th>zcr</th>\n",
       "      <th>zcr.1</th>\n",
       "      <th>zcr.2</th>\n",
       "      <th>zcr.3</th>\n",
       "      <th>zcr.4</th>\n",
       "      <th>zcr.5</th>\n",
       "      <th>zcr.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7.180653</td>\n",
       "      <td>5.230309</td>\n",
       "      <td>0.249321</td>\n",
       "      <td>1.347620</td>\n",
       "      <td>1.482478</td>\n",
       "      <td>0.531371</td>\n",
       "      <td>1.481593</td>\n",
       "      <td>2.691455</td>\n",
       "      <td>0.866868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054125</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.012111</td>\n",
       "      <td>5.758890</td>\n",
       "      <td>0.459473</td>\n",
       "      <td>0.085629</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.089872</td>\n",
       "      <td>0.061448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.888963</td>\n",
       "      <td>0.760539</td>\n",
       "      <td>0.345297</td>\n",
       "      <td>2.295201</td>\n",
       "      <td>1.654031</td>\n",
       "      <td>0.067592</td>\n",
       "      <td>1.366848</td>\n",
       "      <td>1.054094</td>\n",
       "      <td>0.108103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063831</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>2.824694</td>\n",
       "      <td>0.466309</td>\n",
       "      <td>0.084578</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.716724</td>\n",
       "      <td>0.069330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.527563</td>\n",
       "      <td>-0.077654</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>0.685883</td>\n",
       "      <td>1.937570</td>\n",
       "      <td>0.880839</td>\n",
       "      <td>-0.923192</td>\n",
       "      <td>-0.927232</td>\n",
       "      <td>0.666617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040730</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.014759</td>\n",
       "      <td>6.808415</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.053114</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.193303</td>\n",
       "      <td>0.044861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>3.702245</td>\n",
       "      <td>-0.291193</td>\n",
       "      <td>2.196742</td>\n",
       "      <td>-0.234449</td>\n",
       "      <td>1.367364</td>\n",
       "      <td>0.998411</td>\n",
       "      <td>1.770694</td>\n",
       "      <td>1.604566</td>\n",
       "      <td>0.521217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074358</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.013921</td>\n",
       "      <td>21.434212</td>\n",
       "      <td>0.452148</td>\n",
       "      <td>0.077515</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.542325</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.193837</td>\n",
       "      <td>-0.198527</td>\n",
       "      <td>0.201546</td>\n",
       "      <td>0.258556</td>\n",
       "      <td>0.775204</td>\n",
       "      <td>0.084794</td>\n",
       "      <td>-0.289294</td>\n",
       "      <td>-0.816410</td>\n",
       "      <td>0.043851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095003</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.021355</td>\n",
       "      <td>16.669037</td>\n",
       "      <td>0.469727</td>\n",
       "      <td>0.047225</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>3.189831</td>\n",
       "      <td>0.030993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id  chroma_cens  chroma_cens.1  chroma_cens.2  chroma_cens.3  \\\n",
       "0         2     7.180653       5.230309       0.249321       1.347620   \n",
       "1         3     1.888963       0.760539       0.345297       2.295201   \n",
       "2         5     0.527563      -0.077654      -0.279610       0.685883   \n",
       "3        10     3.702245      -0.291193       2.196742      -0.234449   \n",
       "4        20    -0.193837      -0.198527       0.201546       0.258556   \n",
       "\n",
       "   chroma_cens.4  chroma_cens.5  chroma_cens.6  chroma_cens.7  chroma_cens.8  \\\n",
       "0       1.482478       0.531371       1.481593       2.691455       0.866868   \n",
       "1       1.654031       0.067592       1.366848       1.054094       0.108103   \n",
       "2       1.937570       0.880839      -0.923192      -0.927232       0.666617   \n",
       "3       1.367364       0.998411       1.770694       1.604566       0.521217   \n",
       "4       0.775204       0.084794      -0.289294      -0.816410       0.043851   \n",
       "\n",
       "     ...     tonnetz.39  tonnetz.40  tonnetz.41        zcr     zcr.1  \\\n",
       "0    ...       0.054125    0.012226    0.012111   5.758890  0.459473   \n",
       "1    ...       0.063831    0.014212    0.017740   2.824694  0.466309   \n",
       "2    ...       0.040730    0.012691    0.014759   6.808415  0.375000   \n",
       "3    ...       0.074358    0.017952    0.013921  21.434212  0.452148   \n",
       "4    ...       0.095003    0.022492    0.021355  16.669037  0.469727   \n",
       "\n",
       "      zcr.2     zcr.3     zcr.4     zcr.5     zcr.6  \n",
       "0  0.085629  0.071289  0.000000  2.089872  0.061448  \n",
       "1  0.084578  0.063965  0.000000  1.716724  0.069330  \n",
       "2  0.053114  0.041504  0.000000  2.193303  0.044861  \n",
       "3  0.077515  0.071777  0.000000  3.542325  0.040800  \n",
       "4  0.047225  0.040039  0.000977  3.189831  0.030993  \n",
       "\n",
       "[5 rows x 519 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df = pd.read_csv(\"data/features.csv\", skiprows=range(1,4))\n",
    "feature_df.rename(columns={'feature':'track_id'}, inplace=True)\n",
    "genre_df = pd.read_csv(\"data/genres.csv\")\n",
    "track_df = pd.read_csv(\"data/tracks.csv\", skiprows=[0,2])\n",
    "track_df.rename(columns={'Unnamed: 0':'track_id'}, inplace=True)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre_id</th>\n",
       "      <th>#tracks</th>\n",
       "      <th>parent</th>\n",
       "      <th>title</th>\n",
       "      <th>top_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8693</td>\n",
       "      <td>38</td>\n",
       "      <td>Avant-Garde</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5271</td>\n",
       "      <td>0</td>\n",
       "      <td>International</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1752</td>\n",
       "      <td>0</td>\n",
       "      <td>Blues</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4126</td>\n",
       "      <td>0</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4106</td>\n",
       "      <td>0</td>\n",
       "      <td>Classical</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   genre_id  #tracks  parent          title  top_level\n",
       "0         1     8693      38    Avant-Garde         38\n",
       "1         2     5271       0  International          2\n",
       "2         3     1752       0          Blues          3\n",
       "3         4     4126       0           Jazz          4\n",
       "4         5     4106       0      Classical          5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['track_id', 'comments', 'date_created', 'date_released', 'engineer',\n",
       "       'favorites', 'id', 'information', 'listens', 'producer', 'tags',\n",
       "       'title', 'tracks', 'type', 'active_year_begin', 'active_year_end',\n",
       "       'associated_labels', 'bio', 'comments.1', 'date_created.1',\n",
       "       'favorites.1', 'id.1', 'latitude', 'location', 'longitude', 'members',\n",
       "       'name', 'related_projects', 'tags.1', 'website', 'wikipedia_page',\n",
       "       'split', 'subset', 'bit_rate', 'comments.2', 'composer',\n",
       "       'date_created.2', 'date_recorded', 'duration', 'favorites.2',\n",
       "       'genre_top', 'genres', 'genres_all', 'information.1', 'interest',\n",
       "       'language_code', 'license', 'listens.1', 'lyricist', 'number',\n",
       "       'publisher', 'tags.2', 'title.1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options for picking out a genre label: genre_top and genres seem to be good contenders.  Let's look at a sample of these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre_top</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pop</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[76, 103]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  genre_top     genres\n",
       "0   Hip-Hop       [21]\n",
       "1   Hip-Hop       [21]\n",
       "2   Hip-Hop       [21]\n",
       "3       Pop       [10]\n",
       "4       NaN  [76, 103]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_df[['genre_top', 'genres']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genre_top is categorical and has missing values for some tracks.  There are no missing values in the genres column, but its datatype is a list, which would mean we'd have to figure out how to pick a label.  Let's try a different approach using the genre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre_id</th>\n",
       "      <th>#tracks</th>\n",
       "      <th>parent</th>\n",
       "      <th>title</th>\n",
       "      <th>top_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>38</td>\n",
       "      <td>38154</td>\n",
       "      <td>0</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>34413</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>32923</td>\n",
       "      <td>0</td>\n",
       "      <td>Rock</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1235</td>\n",
       "      <td>14938</td>\n",
       "      <td>0</td>\n",
       "      <td>Instrumental</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>13845</td>\n",
       "      <td>0</td>\n",
       "      <td>Pop</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>12706</td>\n",
       "      <td>0</td>\n",
       "      <td>Folk</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>9261</td>\n",
       "      <td>12</td>\n",
       "      <td>Punk</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8693</td>\n",
       "      <td>38</td>\n",
       "      <td>Avant-Garde</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>8389</td>\n",
       "      <td>0</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>7268</td>\n",
       "      <td>38</td>\n",
       "      <td>Noise</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     genre_id  #tracks  parent         title  top_level\n",
       "31         38    38154       0  Experimental         38\n",
       "14         15    34413       0    Electronic         15\n",
       "11         12    32923       0          Rock         12\n",
       "162      1235    14938       0  Instrumental       1235\n",
       "9          10    13845       0           Pop         10\n",
       "16         17    12706       0          Folk         17\n",
       "22         25     9261      12          Punk         12\n",
       "0           1     8693      38   Avant-Garde         38\n",
       "20         21     8389       0       Hip-Hop         21\n",
       "27         32     7268      38         Noise         38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ten_genres = genre_df.sort_values(by='#tracks', ascending=False)[:10]\n",
    "top_ten_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only look at the top ten genres listed and, if the track's top genre is in this list, include that track in our reduced dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock            14182\n",
       "Experimental    10608\n",
       "Electronic       9372\n",
       "Hip-Hop          3552\n",
       "Folk             2803\n",
       "Pop              2332\n",
       "Instrumental     2079\n",
       "Name: genre_top, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_df = track_df[track_df['genre_top'].isin(top_ten_genres['title'].values)]\n",
    "track_df['genre_top'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chroma_cens</th>\n",
       "      <th>chroma_cens.1</th>\n",
       "      <th>chroma_cens.2</th>\n",
       "      <th>chroma_cens.3</th>\n",
       "      <th>chroma_cens.4</th>\n",
       "      <th>chroma_cens.5</th>\n",
       "      <th>chroma_cens.6</th>\n",
       "      <th>chroma_cens.7</th>\n",
       "      <th>chroma_cens.8</th>\n",
       "      <th>chroma_cens.9</th>\n",
       "      <th>...</th>\n",
       "      <th>tonnetz.40</th>\n",
       "      <th>tonnetz.41</th>\n",
       "      <th>zcr</th>\n",
       "      <th>zcr.1</th>\n",
       "      <th>zcr.2</th>\n",
       "      <th>zcr.3</th>\n",
       "      <th>zcr.4</th>\n",
       "      <th>zcr.5</th>\n",
       "      <th>zcr.6</th>\n",
       "      <th>genre_top</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>track_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.180653</td>\n",
       "      <td>5.230309</td>\n",
       "      <td>0.249321</td>\n",
       "      <td>1.347620</td>\n",
       "      <td>1.482478</td>\n",
       "      <td>0.531371</td>\n",
       "      <td>1.481593</td>\n",
       "      <td>2.691455</td>\n",
       "      <td>0.866868</td>\n",
       "      <td>1.341231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012226</td>\n",
       "      <td>0.012111</td>\n",
       "      <td>5.758890</td>\n",
       "      <td>0.459473</td>\n",
       "      <td>0.085629</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.089872</td>\n",
       "      <td>0.061448</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.888963</td>\n",
       "      <td>0.760539</td>\n",
       "      <td>0.345297</td>\n",
       "      <td>2.295201</td>\n",
       "      <td>1.654031</td>\n",
       "      <td>0.067592</td>\n",
       "      <td>1.366848</td>\n",
       "      <td>1.054094</td>\n",
       "      <td>0.108103</td>\n",
       "      <td>0.619185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>2.824694</td>\n",
       "      <td>0.466309</td>\n",
       "      <td>0.084578</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.716724</td>\n",
       "      <td>0.069330</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.527563</td>\n",
       "      <td>-0.077654</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>0.685883</td>\n",
       "      <td>1.937570</td>\n",
       "      <td>0.880839</td>\n",
       "      <td>-0.923192</td>\n",
       "      <td>-0.927232</td>\n",
       "      <td>0.666617</td>\n",
       "      <td>1.038546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.014759</td>\n",
       "      <td>6.808415</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.053114</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.193303</td>\n",
       "      <td>0.044861</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.702245</td>\n",
       "      <td>-0.291193</td>\n",
       "      <td>2.196742</td>\n",
       "      <td>-0.234449</td>\n",
       "      <td>1.367364</td>\n",
       "      <td>0.998411</td>\n",
       "      <td>1.770694</td>\n",
       "      <td>1.604566</td>\n",
       "      <td>0.521217</td>\n",
       "      <td>1.982386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.013921</td>\n",
       "      <td>21.434212</td>\n",
       "      <td>0.452148</td>\n",
       "      <td>0.077515</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.542325</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.918445</td>\n",
       "      <td>0.674147</td>\n",
       "      <td>0.577818</td>\n",
       "      <td>1.281117</td>\n",
       "      <td>0.933746</td>\n",
       "      <td>0.078177</td>\n",
       "      <td>1.199204</td>\n",
       "      <td>-0.175223</td>\n",
       "      <td>0.925482</td>\n",
       "      <td>1.438509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.015819</td>\n",
       "      <td>4.731087</td>\n",
       "      <td>0.419434</td>\n",
       "      <td>0.064370</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.806106</td>\n",
       "      <td>0.054623</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          chroma_cens  chroma_cens.1  chroma_cens.2  chroma_cens.3  \\\n",
       "track_id                                                             \n",
       "2            7.180653       5.230309       0.249321       1.347620   \n",
       "3            1.888963       0.760539       0.345297       2.295201   \n",
       "5            0.527563      -0.077654      -0.279610       0.685883   \n",
       "10           3.702245      -0.291193       2.196742      -0.234449   \n",
       "134          0.918445       0.674147       0.577818       1.281117   \n",
       "\n",
       "          chroma_cens.4  chroma_cens.5  chroma_cens.6  chroma_cens.7  \\\n",
       "track_id                                                               \n",
       "2              1.482478       0.531371       1.481593       2.691455   \n",
       "3              1.654031       0.067592       1.366848       1.054094   \n",
       "5              1.937570       0.880839      -0.923192      -0.927232   \n",
       "10             1.367364       0.998411       1.770694       1.604566   \n",
       "134            0.933746       0.078177       1.199204      -0.175223   \n",
       "\n",
       "          chroma_cens.8  chroma_cens.9    ...      tonnetz.40  tonnetz.41  \\\n",
       "track_id                                  ...                               \n",
       "2              0.866868       1.341231    ...        0.012226    0.012111   \n",
       "3              0.108103       0.619185    ...        0.014212    0.017740   \n",
       "5              0.666617       1.038546    ...        0.012691    0.014759   \n",
       "10             0.521217       1.982386    ...        0.017952    0.013921   \n",
       "134            0.925482       1.438509    ...        0.016322    0.015819   \n",
       "\n",
       "                zcr     zcr.1     zcr.2     zcr.3  zcr.4     zcr.5     zcr.6  \\\n",
       "track_id                                                                       \n",
       "2          5.758890  0.459473  0.085629  0.071289    0.0  2.089872  0.061448   \n",
       "3          2.824694  0.466309  0.084578  0.063965    0.0  1.716724  0.069330   \n",
       "5          6.808415  0.375000  0.053114  0.041504    0.0  2.193303  0.044861   \n",
       "10        21.434212  0.452148  0.077515  0.071777    0.0  3.542325  0.040800   \n",
       "134        4.731087  0.419434  0.064370  0.050781    0.0  1.806106  0.054623   \n",
       "\n",
       "          genre_top  \n",
       "track_id             \n",
       "2           Hip-Hop  \n",
       "3           Hip-Hop  \n",
       "5           Hip-Hop  \n",
       "10              Pop  \n",
       "134         Hip-Hop  \n",
       "\n",
       "[5 rows x 519 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df = feature_df.set_index('track_id').join(track_df[['track_id', 'genre_top']].set_index('track_id'))\n",
    "feature_df.dropna(how='any', axis=0, inplace=True)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our label determined, let's prepare the data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(feature_df['genre_top'].unique(), range(0,10)))\n",
    "y = feature_df['genre_top'].map(label_mapping, na_action='ignore').values\n",
    "X = feature_df.drop(columns=['genre_top'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of numerical columns in the feature_df, so let's use PCA to reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.962243056182637"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X)\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only takes 5 components to achieve 96% explained variance, most likely because the columns are various statistical methods on the same set of values.  Now to split the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35942\n",
      "8986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# I'm adding a random_state so that the results of our shuffle don't change on different runs\n",
    "# of the program\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing an 80/20 train/test split is appropriate for our dataset because the data points aren't related to each other in time.  Shuffling the dataset is especially necessary because the data includes multiple tracks from the same album that are sequentially listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s pickle this data to ensure we are always get the same data from our split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_train, open( 'pickledData/X_train.p', 'wb' ))\n",
    "pickle.dump(X_test, open( 'pickledData/X_test.p', 'wb' ))\n",
    "pickle.dump(y_train, open( 'pickledData/y_train.p', 'wb' ))\n",
    "pickle.dump(y_test, open( 'pickledData/y_test.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = pickle.load(open( 'pickledData/X_train.p', 'rb' ))\n",
    "X_test = pickle.load(open( 'pickledData/X_test.p', 'rb' ))\n",
    "y_train = pickle.load(open( 'pickledData/y_train.p', 'rb' ))\n",
    "y_test = pickle.load(open( 'pickledData/y_test.p', 'rb' ))\n",
    "\n",
    "\n",
    "scl = StandardScaler()\n",
    "X_train_scl = scl.fit_transform(X_train)\n",
    "X_test_scl = scl.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2.0 Modeling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.1 One-Versus-All Logistic Regression Classifier</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by making a binary logistic regression class for steepest descent, stochastic and newton method logistic regressions. The classes will each accept an eta for step size and a number of iterations for which the regression will run. The user will be able to pass a keyword on the parameter reg into the class to determine which regularization method he wants to use on the data, “L1”, “L2”, “Both”, \"None\". It will start with \"None\" by default. We can make these classes based on the templetes we established in class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Binary Steepest Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BinaryLogisticRegressionBase: #base class, never does anything just inherit from it\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20, reg='None', C=0.001): #eta how big a step do I take #iterations how many steps\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.reg = reg\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self): #what you get when you call a string\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term, add 1s column to add bias \n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True): #probabilities of each class what is the probability that a class of this instance = 1\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1 w transpose x\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y): #SLOW\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        \n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference how much you update the x value\n",
    "       #if you y value is very different than predict_proba value, you can add another weight to determine how much this will effect the gradient. rarer classes get a higher weight \n",
    "       #assign the weights by class\n",
    "        #use it to be more senstive to certin classes\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        #[:,np.newaxis] makes this into a column vector \n",
    "        return gradient.reshape(self.w_.shape)\n",
    "\n",
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        \n",
    "        \n",
    "        # but keep other keywords\n",
    "        super().__init__(C=self.C, **kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        \n",
    "        if(self.reg == 'L2'): #detect regularization\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        if(self.reg == 'L1'):\n",
    "            gradient[1:] += -1 * np.sign(self.w_[1:]) * self.C\n",
    "        if(self.reg == 'Both'):\n",
    "            gradient[1:] += (-2 * self.w_[1:] * self.C) + (-1 * np.sign(self.w_[1:]) * self.C)\n",
    "            \n",
    "        return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Binary Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.reg == 'L2'): #detect regularization\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        if(self.reg == 'L1'):\n",
    "            gradient[1:] += -1 * np.sign(self.w_[1:]) * self.C\n",
    "        if(self.reg == 'Both'):\n",
    "            gradient[1:] += (-2 * self.w_[1:] * self.C) + (-1 * np.sign(self.w_[1:]) * self.C)\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Binary Newton's method (BFGS) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function_L2(w,X,y,C): #optimized for L2\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    @staticmethod\n",
    "    def objective_function_L1(w,X,y,C): #optimized for L1\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(np.absolute(w)) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    @staticmethod\n",
    "    def objective_function_Both(w,X,y,C): #optimized for L1 and L2\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) + C*sum(np.absolute(w)) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    @staticmethod\n",
    "    def objective_function_None(w,X,y,C): #no regularization\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0]))  #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient_No_Reg(w,X,y,C): #no regularization\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        return -gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_gradient_L1_Reg(w,X,y,C): #L1 regularization\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        \n",
    "        gradient[1:] += -1 * np.sign(w[1:]) * C\n",
    "        return -gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_gradient_L2_Reg(w,X,y,C): #L2 regularization\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        \n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_gradient_Both_Reg(w,X,y,C): #L1 and L2 regularization\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += (-2 * w[1:] * C) + (-1 * np.sign(w[1:]) * C)\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "       \n",
    "        num_samples, num_features = Xb.shape\n",
    "        if(self.reg == 'None'):\n",
    "            self.w_ = fmin_bfgs(self.objective_function_None, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient_No_Reg, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        if(self.reg == 'L1'):\n",
    "            self.w_ = fmin_bfgs(self.objective_function_L1, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient_L1_Reg, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        if(self.reg == 'L2'):\n",
    "            self.w_ = fmin_bfgs(self.objective_function_L2, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient_L2_Reg, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        if(self.reg == 'Both'):\n",
    "            self.w_ = fmin_bfgs(self.objective_function_Both, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient_Both_Reg, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Multiclass Steepest Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, reg = \"None\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.reg = reg\n",
    "        \n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters,self.reg)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C, reg=self.reg,)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Multiclass Stochastic Gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassStochasticLogisticRegression():\n",
    "    def __init__(self, eta, iterations=20, C=0.0,reg = \"None\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.reg = reg\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            SLR = StochasticLogisticRegression(eta=self.eta, reg = self.reg,\n",
    "                                                      iterations=self.iters)\n",
    "            SLR.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(SLR)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for SLR in self.classifiers_:\n",
    "            probs.append(SLR.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Multiclass Binary Newton's method (BFGS) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, reg = \"None\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.reg = reg\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            hblr = BFGSBinaryLogisticRegression(eta = self.eta,iterations = self.iters,reg = self.reg, C=self.C)\n",
    "            hblr.fit(X=X,y=y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created multiclass classes for each of our methods, we can create one logistic regression class which takes in as parameters, the eta, the number of iterations, the c value, the regularization technique to use, and the method to use. It will take the method as the last parameter as will have the options of “Steep” for steepest descent, “Stochastic” for stochastic gradient descent, and “Newton” for Newton’s method. From here the class will construct the appropriate object and use it to fit and predict the data accordingly.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralLogisticRegression:\n",
    "    def __init__(self, eta, iterations=100, C=0.0001, reg = \"None\", method=\"Steep\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.reg = reg\n",
    "        if method == \"Steep\":\n",
    "            self.LRObj = RegularizedLogisticRegression(C=self.C, eta = self.eta, iterations=self.iters, reg = self.reg)\n",
    "        if method == \"Stochastic\":\n",
    "            self.LRObj = MultiClassStochasticLogisticRegression(C=self.C, eta = self.eta, iterations=self.iters, reg = self.reg)\n",
    "        if method == \"Newton\":\n",
    "            self.LRObj = MultiClassLogisticRegression(C=self.C, eta = self.eta, iterations=self.iters, reg = self.reg)\n",
    "            \n",
    "    def fit(self,X,y):\n",
    "        self.LRObj.fit(X,y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.LRObj.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our multi variable general logistic regression class, let’s run some test to find out which model and C-value will give us optimal results. We will set out eta value to .1, apply both L2 regularizations across our data, run 100 iterations, and vary the C value to see how it changes our accuracy. We will start with a C value of .001 and increase this value by .001 over 10 iterations. We will train our equation on the train data then run it against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl = StandardScaler()\n",
    "X_train_scl = scl.fit_transform(X_train)\n",
    "X_test_scl = scl.transform(X_test)\n",
    "def C_Vary(methodType):\n",
    "    accuracy = []\n",
    "    C_Value = .001\n",
    "    for _ in range(0,10):\n",
    "        a = GeneralLogisticRegression(eta=.1, C = C_Value, method=methodType, reg=\"L2\")\n",
    "        a.fit(X_train_scl, y_train)\n",
    "        yhat = a.predict(X_test_scl)\n",
    "        accuracy.append(accuracy_score(y_test,yhat))\n",
    "        C_Value = C_Value +.001\n",
    "    return accuracy\n",
    "\n",
    "Stochastic_Accuracy = C_Vary(\"Stochastic\")\n",
    "Steep_Accuracy = C_Vary(\"Steep\")\n",
    "Newton_Accuracy = C_Vary(\"Newton\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Stochastic_Accuracy, open( 'pickledData/Stochastic_Accuracy.p', 'wb' ))\n",
    "pickle.dump(Steep_Accuracy, open( 'pickledData/Steep_Accuracy.p', 'wb' ))\n",
    "pickle.dump(Newton_Accuracy, open( 'pickledData/Newton_Accuracy.p', 'wb' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stochastic_Accuracy = pickle.load(open( 'pickledData/Stochastic_Accuracy.p', 'rb' ))\n",
    "Steep_Accuracy = pickle.load(open( 'pickledData/Steep_Accuracy.p', 'rb' ))\n",
    "Newton_Accuracy = pickle.load(open( 'pickledData/Newton_Accuracy.p', 'rb' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2aab2faeb8d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aab2fd6ce48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C_Values = [.001,.002,.003,.004,.005,.006,.007,.008,.009,.010]\n",
    "converter = pd.Series(Stochastic_Accuracy)\n",
    "df = pd.DataFrame()\n",
    "df['Stochastic'] = converter.values\n",
    "\n",
    "converter = pd.Series(Steep_Accuracy)\n",
    "df['Steep'] = converter.values\n",
    "\n",
    "converter = pd.Series(Newton_Accuracy)\n",
    "df['Newton'] = converter.values\n",
    "\n",
    "converter = pd.Series(C_Values)\n",
    "df['C_Values'] = converter.values\n",
    "fig1 = df.plot(kind='bar', x='C_Values', legend=True, title='Accuracy of Various Logistic Methods Across Multiple C Values',figsize=(20,35))\n",
    "fig1.legend(prop={'size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the results of our 10 trials, we can see that the Steep and Newton methods tend to work better than the Stochastic method consistently for our data. Both the Steep and Newton had the highest accuracy when ran with a Cost Value of .001. However all of this analysis exists on the caveat that additional C values may prove the value used for Cost is not very important, as we do a bigger and bigger grid search across more and more C values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s run the scikit-learn’s Newton regression under the same set of parameters and see how our equation compares. We will run each equation 10 times, measuring the speed and accuracy of each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sci_speed = []\n",
    "sci_accuracy = []\n",
    "our_speed =[]\n",
    "our_accuracy = []\n",
    "\n",
    "for _ in range(50):\n",
    "    #scikit data crunching\n",
    "    start = time.time()\n",
    "    lr_sk = LogisticRegression(solver='lbfgs',C = .001)\n",
    "    lr_sk.fit(X_train, y_train) \n",
    "    sci_hat = lr_sk.predict(X_test)\n",
    "    sci_acc = accuracy_score(y_test,sci_hat)\n",
    "    end = time.time()\n",
    "    sci_time = start-end\n",
    "    sci_speed.append(sci_time)\n",
    "    sci_accuracy.append(sci_acc)\n",
    "    #our data crunching\n",
    "    start = time.time()\n",
    "    a = GeneralLogisticRegression(eta=.1, C = .001, method='Newton', reg=\"L2\")\n",
    "    a.fit(X_train_scl, y_train)\n",
    "    yhat = a.predict(X_test_scl)\n",
    "    our_acc = accuracy_score(y_test,yhat)\n",
    "    end = time.time()\n",
    "    our_time = start-end\n",
    "    our_speed.append(our_time)\n",
    "    our_accuracy.append(our_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sci_speed, open( 'pickledData/sci_speed.p', 'wb' ))\n",
    "pickle.dump(sci_accuracy, open( 'pickledData/sci_accuracy.p', 'wb' ))\n",
    "pickle.dump(our_speed, open( 'pickledData/our_speed.p', 'wb' ))\n",
    "pickle.dump(our_accuracy, open( 'pickledData/our_accuracy.p', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_speed = pickle.load(open( 'pickledData/sci_speed.p', 'rb' ))\n",
    "sci_accuracy = pickle.load(open( 'pickledData/sci_accuracy.p', 'rb' ))\n",
    "our_speed = pickle.load(open( 'pickledData/our_speed.p', 'rb' ))\n",
    "our_accuracy = pickle.load(open( 'pickledData/our_accuracy.p', 'rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x2aab2e1c43c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aab2e1c44e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sci_df = pd.DataFrame()\n",
    "converter = pd.Series(sci_speed)\n",
    "Sci_df['Time'] = converter.values\n",
    "Sci_df['Time'] = Sci_df['Time'] * -1\n",
    "converter = pd.Series(sci_accuracy)\n",
    "Sci_df['Accuracy'] = converter.values\n",
    "Sci_df['Type'] = 'Scikit-Learn'\n",
    "\n",
    "Our_df = pd.DataFrame()\n",
    "converter = pd.Series(our_speed)\n",
    "Our_df['Time'] = converter.values\n",
    "Our_df['Time'] = Our_df['Time'] * -1\n",
    "converter = pd.Series(our_accuracy)\n",
    "Our_df['Accuracy'] = converter.values\n",
    "Our_df['Type'] = 'Our_Algorithm'\n",
    "frames = [Sci_df,Our_df]\n",
    "df = pd.concat(frames)\n",
    "groups = df.groupby('Type')\n",
    "#fig1 = df.plot(kind='scatter', x='Time',y='Accuracy', legend=True, title='Accuracy of Various Logistic Methods Across Multiple C Values')\n",
    "# fig1.legend(prop={'size': 18})\n",
    "# fig, ax = plt.subplots()\n",
    "# for name, group in groups:\n",
    "#     ax.plot(group.Time, x='Time', y = 'Accuracy', group.Accuracy, marker='o', linestyle='', ms=12, label=name)\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "sns.pairplot(y_vars=[\"Accuracy\"], x_vars=[\"Time\"], data=df, hue=\"Type\", size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both algorithms tend to have consistent overall performance in accuracy.Our Newton’s method model runs at a similar but lower accuracy to the scikit-learn model but does it in approximately 40% of the time. Their accuracy tends to be approximately 14% higher than our algorithm. However our model runs in the range of 7.5-1.0 seconds and the scikit-learn model runs in the 25.0-27.0 seconds range. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen from our previous trials, the Newton’s method tends to run the most accurately. Our model tends to run with an accuracy of approximately 56% while scikit-learn boast an approximate accuracy of 70%. However, neither of these models can compete with the LPNKK1 model from MIREX which boast an accuracy score of 77%.. For a platform based on searching for music such as Spotify, the accuracy of an algorithm is ultimately the most important factor. If a song is mis-categorized it cannot be found and played by those interested in it, and is instead found by those who are not. This leads to less plays and less money for the artist overall. In conclusion, Spotify should deploy an LPNKK1 algorithm from MIREX instead of our or scikit-learn’s Newton Logistic Regression for the categorization of their content. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
