{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Lab Six: Convolutional Network Architectures </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Adam Ashcraft, Maya Muralidhar, Nora Potenti, Michael Watts </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.0 Preparation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.1 Business Understanding </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset of approximately 4,300 images of flowers. These images have been taken from Flickr, Google, and Yandax and are divided into five categories: dasies, dandelions, roses, sunflowers, and tulips. The images were collected by performing a data scrape of these image hosting services by the dataset creator Alexander Mamaev. The images are in standard definition quality and do not have a standardized size. According to the Kaggle page, the data was collected to answer the question, “What kind of flower is that?” The creator also includes a prediction task for the data on Kaggle, “You can use this dataset to recognize plants from the photo.” \n",
    "\n",
    "This tool would primary serve the completely flower illiterate. It provides a very basic stepping stone into the wide world of botany for those with no experience in the field. With an application like this, a user will be able to successfully differentiate between different types of flowers based on subtler nuances than simply factors like color. Ideally, this would prevent someone from buying sunflowers instead of tulips simply because they are the same color. \n",
    "\n",
    "\n",
    "Another plant photo identification tool already exists in the market, Plantsnap. According to the Plantsnap website, it boasts a 96% success rate. In order for our tool to be considered viable, it must have at least, if not greater than, a 96% success rate.  \n",
    "\n",
    "<hr>\n",
    "Kaggle dataset: https://www.kaggle.com/alxmamaev/flowers-recognition <br>\n",
    "Plantsnap self reported success rate: https://plantsnap.com/faq/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.2 Evaluation Criteria </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we determine our evaluation criteria, let’s take a moment to look at our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayam\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, Conv3D, MaxPooling2D, MaxPooling3D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from scipy.misc import imresize\n",
    "from sklearn import metrics as mt\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use glob to get all file names in each folder\n",
    "daisyFiles = glob.glob(\"flowers/daisy/*\")\n",
    "dandelionFiles = glob.glob(\"flowers/dandelion/*\")\n",
    "roseFiles = glob.glob(\"flowers/rose/*\")\n",
    "sunflowerFiles = glob.glob(\"flowers/sunflower/*\")\n",
    "tulipFiles = glob.glob(\"flowers/tulip/*\")\n",
    "\n",
    "daisies = []\n",
    "dandelions = []\n",
    "roses = []\n",
    "sunflowers = []\n",
    "tulips = []\n",
    "#function to take image name, open image as numpy array and add to list\n",
    "def addImageToArray(imageFileArray, imageArray):\n",
    "    for file in imageFileArray:\n",
    "        imageArray.append(np.asarray(Image.open(file)))\n",
    "        \n",
    "addImageToArray(daisyFiles,daisies)\n",
    "addImageToArray(dandelionFiles,dandelions)\n",
    "addImageToArray(roseFiles,roses)\n",
    "addImageToArray(sunflowerFiles,sunflowers)\n",
    "addImageToArray(tulipFiles,tulips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToGrayScale(RGBArray):\n",
    "    return np.dot(RGBArray[...,:3], [0.3, 0.59, 0.11])\n",
    "def convertAllImagesToGrey(ImageArray):\n",
    "    for i in range (0, len(ImageArray)):\n",
    "        ImageArray[i] = convertToGrayScale(ImageArray[i])\n",
    "convertAllImagesToGrey(daisies)\n",
    "convertAllImagesToGrey(dandelions)\n",
    "convertAllImagesToGrey(roses)\n",
    "convertAllImagesToGrey(sunflowers)\n",
    "convertAllImagesToGrey(tulips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x20a2eab0a58>,\n",
       "  <matplotlib.patches.Wedge at 0x20a2eac0198>,\n",
       "  <matplotlib.patches.Wedge at 0x20a2eac0898>,\n",
       "  <matplotlib.patches.Wedge at 0x20a2eac0f98>,\n",
       "  <matplotlib.patches.Wedge at 0x20a2eac86d8>],\n",
       " [Text(0.932655,0.583228,'Daisy'),\n",
       "  Text(-0.337028,1.0471,'Dandelion'),\n",
       "  Text(-1.09692,-0.08226,'Rose'),\n",
       "  Text(-0.421044,-1.01623,'Sunflower'),\n",
       "  Text(0.830539,-0.721253,'Tulip')],\n",
       " [Text(0.508721,0.318124,'17.8%'),\n",
       "  Text(-0.183834,0.571144,'24.3%'),\n",
       "  Text(-0.59832,-0.0448691,'18.1%'),\n",
       "  Text(-0.22966,-0.554307,'17.0%'),\n",
       "  Text(0.453021,-0.393411,'22.8%')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pieValues = [len(daisies), len(dandelions), len(roses), len(sunflowers), len(tulips)]\n",
    "plt.pie(x=pieValues,labels=['Daisy', 'Dandelion', 'Rose', 'Sunflower','Tulip'], autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A false negative and a false positive both represent the same idea here, an erroneous categorization of the flower for our user. Considering the fact, we are working with total flower novices, we will want to minimize both our false positive and false negatives rates as much as possible. We cannot assume our user is knowledgeable enough to detect them himself. We must also take into account the slight class imbalance we see in the above graphic. Dandelions and Tulips represent almost 50% of the classes in the data set. In order to focus on minimizing both the false positives and false negatives as well consider the class imbalance, we will use F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "croppedDaisies = [imresize(x,size=(330,500), interp='nearest') for x in daisies]    \n",
    "\n",
    "croppedDandelions = [imresize(x,size=(330,500), interp='nearest') for x in dandelions]   \n",
    "\n",
    "croppedRoses =[imresize(x,size=(330,500), interp='nearest') for x in roses]  \n",
    "\n",
    "croppedSunflowers = [imresize(x,size=(330,500), interp='nearest') for x in sunflowers]    \n",
    "\n",
    "croppedTulips = [imresize(x,size=(330,500), interp='nearest') for x in tulips] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenMatrix(arrayOfMatracies):\n",
    "    flattenedArray = []\n",
    "    for m in arrayOfMatracies:\n",
    "        flattenedArray.append(m.flatten())\n",
    "        \n",
    "    return flattenedArray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.2 Data Division </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data division, we will do a stratified shuffle train test split across our data then use a stratified shuffle split. The stratification will correct for the class imbalance of the data. The data is currently grouped into 5 groups, 1 per class. This is not an accurate reflection of how our model will be used. It is highly unlikely that the application will only ever see one kind of flower at a time, iteration after iteration. It will more than likely be exposed to a wide variety of flower with every use. In order to better model this, the data will be shuffled in order to split the groups of classes up and distribute them among the k-folds. The train-test split will give us a section of data never before seen by our model we can use to easily check its performance against. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.5, random_state=1)\n",
    "X = np.array(croppedDaisies + croppedTulips + croppedDandelions + croppedRoses + croppedSunflowers)\n",
    "y = np.array(len(croppedDaisies)*[0] + len(croppedTulips)*[1] + len(croppedDandelions)*[2] + len(croppedRoses)*[3] + len(croppedSunflowers)*[4])\n",
    "X_split, X_test, y_split, y_test = train_test_split(X, y, stratify = y, shuffle = True, random_state=42, test_size=0.2)\n",
    "\n",
    "X_test_resize = [imresize(x,size=(33,50), interp='nearest') for x in X_test]\n",
    "\n",
    "X_test_resize = np.stack(X_test_resize,axis=0)\n",
    "X_test_resize = flattenMatrix(X_test_resize)\n",
    "\n",
    "\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2.0 Convolutional Neural Nets </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.1 Data Expansion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to assume that our photographer has no idea what features he is trying to capture in the image. This means he does not know good angles, lighting, or focus for his flower photograph. In order to mirror an amateur as much as possible with our data, we will create a wide variety of augmentations with our data generator.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False, #adding to each pixel\n",
    "    samplewise_std_normalization=False, #adding to each picture\n",
    "    zca_whitening=False,\n",
    "    rotation_range=180, # used, Int. Degree range for random rotations.\n",
    "    width_shift_range=0.5, # used, Float (fraction of total width). Range for random horizontal shifts.\n",
    "    height_shift_range=0.5, # used,  Float (fraction of total height). Range for random vertical shifts.\n",
    "    shear_range=0., # Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.2 Simple CNN </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_CLASSES = 5 \n",
    "\n",
    "print(len(X_test_resize[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "mlp = Sequential()\n",
    "mlp.add( Dense(input_dim=len(X_test_resize[0]), units=100, activation='relu') )\n",
    "mlp.add( Dense(units=50, activation='relu') )\n",
    "mlp.add( Dense(NUM_CLASSES) )\n",
    "mlp.add( Activation('softmax') )\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "count = 0\n",
    "for train_index, test_index in sss.split(X_split, y_split): #SSS k folds, will run and print F1 score for each\n",
    "    count = count + 1\n",
    "    print('Fold #', count, ':')\n",
    "    X_train, X_test = X_split[train_index], X_split[test_index]\n",
    "    y_train, y_test = y_split[train_index], y_split[test_index]\n",
    "    \n",
    "    NUM_CLASSES = 5 \n",
    "    X_train_resize = [imresize(x,size=(33,50), interp='nearest') for x in X_train]\n",
    "    X_train_resize = np.stack(X_train_resize,axis=0)\n",
    "    X_train_resize = flattenMatrix(X_train_resize)\n",
    "\n",
    "    X_test_resize = [imresize(x,size=(33,50), interp='nearest') for x in X_test]\n",
    "    X_test_resize = np.stack(X_test_resize,axis=0)\n",
    "    X_test_resize = flattenMatrix(X_test_resize)\n",
    "    y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "\n",
    "    info = mlp.fit(np.array(X_train_resize), y_train_ohe, \n",
    "        batch_size=32, epochs=150, validation_split=0.33,\n",
    "        shuffle=True, verbose=0)\n",
    "    y_hat = mlp.predict(np.array(X_test_resize))\n",
    "    y_hat_ohe = keras.utils.to_categorical(y_hat.argmax(axis=1), 5)\n",
    "\n",
    "    score = mt.f1_score(y_test_ohe, y_hat_ohe, average='weighted')\n",
    "    print('F1 Score is ', score)\n",
    "    plt.plot(info.history['acc'])\n",
    "    plt.plot(info.history['val_acc'])\n",
    "    plt.title('Accuracy over iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Data', 'Test Data'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save('large_data/mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = load_model('large_data/mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Reshape((1,33,50), input_shape=(1,1650)))\n",
    "cnn.add(Conv2D(filters=16, kernel_size= (10, 10), padding='same', input_shape=(3,33,50)))\n",
    "cnn.add(Conv2D(filters=24, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding='same', \n",
    "                    activation='relu'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(3, 5), data_format=\"channels_first\"))\n",
    "# add one layer on flattened output\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(100, activation='relu'))\n",
    "cnn.add(Dropout(0.5)) # add some dropout for regularization, again!\n",
    "cnn.add(Dense(NUM_CLASSES))\n",
    "cnn.add(Activation('softmax'))\n",
    "\n",
    "# Let's train the model \n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['accuracy'])\n",
    "count = 0\n",
    "for train_index, test_index in sss.split(X_split, y_split): #SSS k folds, will run and print F1 score for each\n",
    "    count = count + 1\n",
    "    print('Fold #', count, ':')\n",
    "    X_train, X_test = X_split[train_index], X_split[test_index]\n",
    "    y_train, y_test = y_split[train_index], y_split[test_index]\n",
    "    \n",
    "    NUM_CLASSES = 5 \n",
    "\n",
    "    X_train_resize = [imresize(x,size=(33,50), interp='nearest') for x in X_train]\n",
    "\n",
    "#     X_train_resize = np.stack(X_train_resize,axis=0)\n",
    "\n",
    "    X_train_resize = flattenMatrix(X_train_resize)\n",
    "\n",
    "    X_test_resize = [imresize(x,size=(33,50), interp='nearest') for x in X_test]\n",
    " \n",
    "    \n",
    "#     X_test_resize = np.stack(X_test_resize,axis=0)\n",
    "    X_test_resize = flattenMatrix(X_test_resize)\n",
    "\n",
    "    y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "   # we need to exapnd the dimensions here to give the \n",
    "#   \"channels\" dimension expected by Keras \n",
    "    info = cnn.fit(np.expand_dims(X_train_resize, axis=1), y_train_ohe, \n",
    "        batch_size=32, validation_split=0.33, epochs=150, \n",
    "        shuffle=True, verbose=0)\n",
    "    yhat = np.argmax(cnn.predict(np.expand_dims(X_test_resize, axis=1)), axis=1)\n",
    "    y_hat_ohe = keras.utils.to_categorical(y_hat.argmax(axis=1), 5)\n",
    "\n",
    "    score = mt.f1_score(y_test_ohe, y_hat_ohe, average='weighted')\n",
    "    print('F1 Score is ', score)\n",
    "\n",
    "    plt.plot(info.history['acc'])\n",
    "    plt.plot(info.history['val_acc'])\n",
    "    plt.title('Accuracy over iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Data', 'Test Data'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('large_data/cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = load_model('large_data/cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def compare_mlp_cnn(cnn, mlp, X_test, y_test):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    if cnn is not None:\n",
    "        yhat_cnn = np.argmax(cnn.predict(np.expand_dims(X_test, axis=1)), axis=1)\n",
    "        f_cnn = mt.f1_score(y_test,yhat_cnn, average= 'weighted')\n",
    "        plt.subplot(1,2,1)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_cnn)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f')\n",
    "        plt.title('CNN: '+str(f_cnn))\n",
    "    \n",
    "    if mlp is not None:\n",
    "        yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "        f_mlp = mt.f1_score(y_test,yhat_mlp, average= 'weighted')\n",
    "        plt.subplot(1,2,2)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_mlp)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2f')\n",
    "        plt.title('MLP: '+str(f_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(cnn,mlp, np.array(X_test_resize),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "inception_model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3458, 33, 50, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(865, 33, 50, 1)\n",
      "(865, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train_up = [imresize(x,size=(33,50,1), interp='nearest') for x in X_split]\n",
    "X_train_up = np.stack(X_train_up,axis=0).reshape((3458, 33, 50, 1))\n",
    "print(X_train_up.shape)\n",
    "X_test_up = [imresize(x,size=(33,50,1), interp='nearest') for x in X_test]\n",
    "X_test_up = np.stack(X_test_up,axis=0).reshape((865, 33, 50, 1))\n",
    "print(X_test_up.shape)\n",
    "y_train_ohe = keras.utils.to_categorical(y_split, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "print(y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (None, None, 3) but got array with shape (33, 50, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-92efeb47525e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_try\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_up\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_try\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_try\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minception_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_try\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (None, None, 3) but got array with shape (33, 50, 1)"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "x_try = np.expand_dims(X_train_up[0], axis=0)\n",
    "x_try = preprocess_input(x_try)\n",
    "preds = inception_model.predict(x_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (None, None, 3) but got array with shape (33, 50, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6eececf8fa6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adamax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_ohe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (None, None, 3) but got array with shape (33, 50, 1)"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "# connect new layers to the output\n",
    "\n",
    "x_inception = inception_model.output\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "x_inception = Dense(1024, activation='relu',kernel_initializer='he_uniform')(x_inception)\n",
    "# and a fully connected layer \n",
    "predictions = Dense(NUM_CLASSES, activation='softmax', kernel_initializer='glorot_uniform')(x_inception)\n",
    "\n",
    "model = Model(inputs=inception_model.input, outputs=predictions)\n",
    "\n",
    "for layer in inception_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer='adamax', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_up,y_train_ohe,epochs=1,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.3 Resnet </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.regularizers import l2\n",
    "# now lets use the LeNet architecture with batch norm\n",
    "# We will also use ReLU where approriate and drop out \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Add\n",
    "from keras.models import Input, Model\n",
    "img_w = 33\n",
    "img_h = 50\n",
    "l2_lambda = 0.000001\n",
    "input_holder = Input(shape=(1, img_w, img_h ))\n",
    "\n",
    "# start with a conv layer\n",
    "x = Conv2D(filters=32,\n",
    "               input_shape = (1, img_w,img_h),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               data_format=\"channels_first\")(input_holder)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(x)\n",
    "\n",
    "x = Conv2D(filters=32,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               data_format=\"channels_first\")(x)\n",
    "\n",
    "x_split = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")(x)\n",
    "\n",
    "x = Conv2D(filters=64,\n",
    "               kernel_size=(1,1),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               data_format=\"channels_first\")(x_split)\n",
    "\n",
    "x = Conv2D(filters=64,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               data_format=\"channels_first\")(x)\n",
    "\n",
    "x = Conv2D(filters=32,\n",
    "               kernel_size=(1,1),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu', \n",
    "               data_format=\"channels_first\")(x)\n",
    "\n",
    "# now add back in the split layer, x_split (residual added in)\n",
    "x = Add()([x, x_split])\n",
    "x = Activation(\"relu\")(x)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(256)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(5)(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "resnet = Model(inputs=input_holder,outputs=x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='adam', # 'adadelta' 'rmsprop'\n",
    "                metrics=[ 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for train_index, test_index in sss.split(X_split, y_split): #SSS k folds, will run and print F1 score for each\n",
    "    count = count + 1\n",
    "    print('Fold #', count, ':')\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#     print(X_train[0].shape)\n",
    "    X_train = [imresize(x,size=(33,50), interp='nearest') for x in X_train]\n",
    "    X_test = [imresize(x,size=(33,50), interp='nearest') for x in X_test]\n",
    "#     X_train = np.stack(X_train,axis=0)\n",
    "#    X_train = flattenMatrix(X_train)\n",
    "#     X_test = np.stack(X_test,axis=0)\n",
    "#    X_test = flattenMatrix(X_test)\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    X_test = (np.expand_dims(X_test, axis=1))\n",
    "    X_train = (np.expand_dims(X_train, axis=1))\n",
    "\n",
    "    y_train_ohe = keras.utils.to_categorical(y_train, 5)\n",
    "    y_test_ohe = keras.utils.to_categorical(y_test, 5)\n",
    "    info = resnet.fit_generator(datagen.flow(X_train, y_train_ohe, batch_size=500), \n",
    "                      steps_per_epoch=int(len(X_train)/100), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=0,\n",
    "                      validation_data=(X_test,y_test_ohe)\n",
    "                     )\n",
    "    y_hat = resnet.predict(X_test)\n",
    "\n",
    "#    print(keras.backend.argmax(y_hat, axis=-1))\n",
    "    y_hat_ohe = keras.utils.to_categorical(y_hat.argmax(axis=1), 5)\n",
    "#     print(y_hat[0])\n",
    "#     print(y_hat[0].shape)\n",
    "#     print(y_hat_ohe[0])\n",
    "#     print(y_hat_ohe[0].shape)\n",
    "#     print(y_test_ohe[0])\n",
    "#     print(y_test_ohe[0].shape)\n",
    "    score = mt.f1_score(y_test_ohe, y_hat_ohe, average='weighted')\n",
    "    print('F1 Score is ', score)\n",
    "    plt.plot(info.history['acc'])\n",
    "    plt.plot(info.history['val_acc'])\n",
    "    plt.title('Accuracy over iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Data', 'Test Data'])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.save('large_data/resnet_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = load_model('large_data/resnet_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_mlp_cnn2(cnn, mlp, X_test, X_test_resnet, y_test): #new one since input for this is different than other CNN\n",
    "    plt.figure(figsize=(15,5))\n",
    "    if cnn is not None:\n",
    "        yhat_cnn = np.argmax(cnn.predict(X_test_resnet), axis=1)\n",
    "        f_cnn = mt.f1_score(y_test,yhat_cnn, average= 'weighted')\n",
    "        plt.subplot(1,2,1)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_cnn)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f')\n",
    "        plt.title('CNN: '+str(f_cnn))\n",
    "    \n",
    "    if mlp is not None:\n",
    "        yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "        f_mlp = mt.f1_score(y_test,yhat_mlp, average= 'weighted')\n",
    "        plt.subplot(1,2,2)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_mlp)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2f')\n",
    "        plt.title('MLP: '+str(f_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_resize_resnet = [imresize(x,size=(33,50), interp='nearest') for x in X_test]\n",
    "X_test_resize_resnet = (np.expand_dims(X_test_resize_resnet, axis=1))\n",
    "compare_mlp_cnn2(resnet,mlp, np.array(X_test_resize), np.array(X_test_resize_resnet),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
