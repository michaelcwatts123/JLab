{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lab Three: Extending Logistic Regression</h1>\n",
    "<b>By Michael Watts, Maya Muralidhar, Nora Potenti, and Adam Ashcraft </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Business Understanding\n",
    "\n",
    "This dataset is from the Free Music Archive, a collection of legally available audio files.  The numerical data is made up of features extracted from a musical analysis python package called librosa that quantifies some of the characteristics of an mp3 and also includes statistics such as mean, skew, and kurtosis.  The dataset also includes unique codes for genres of music.  We will use these features to determine what the genre of a piece is given the librosa feature extraction.  This classification would prove useful for a music streaming application such as Spotify that would want to integrate new music into its platform quickly, especially if the defined genre in the audio file's metadata doesn't matches one of the genres defined in the application's database. It would also help with the application's recommendation system; by broadly defining the main genre categories, users could receive recommendations that are audibly similar. For this use case, the model would be deployed to a production. To measure our success, we will compare our results to the International Society for Music Information Retrieval’s Music Information Retrieval Evaluation eXchange (MIREX) 2017 competition winning algorithm LPNKK1, which has an accuracy of approximately 77%. Our algorithm is considered a success if it can outperform this academic algorithm’s genre classification success rate.\n",
    "<hr>\n",
    "Data Set Source: https://github.com/mdeff/fma <br>\n",
    "MIREX Competition Results: http://www.music-ir.org/nema_out/mirex2017/results/act/mixed_report/summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv(\"data/features.csv\", skiprows=range(1,4))\n",
    "feature_df.rename(columns={'feature':'track_id'}, inplace=True)\n",
    "genre_df = pd.read_csv(\"data/genres.csv\")\n",
    "track_df = pd.read_csv(\"data/tracks.csv\", skiprows=[0,2])\n",
    "track_df.rename(columns={'Unnamed: 0':'track_id'}, inplace=True)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df[['genre_top', 'genres']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_genres = genre_df.sort_values(by='#tracks', ascending=False)[:10]\n",
    "top_ten_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only look at the top ten genres listed and, if the track's top genre is in this list, include that track in our reduced dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = track_df[track_df['genre_top'].isin(top_ten_genres['title'].values)]\n",
    "track_df['genre_top'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.set_index('track_id').join(track_df[['track_id', 'genre_top']].set_index('track_id'))\n",
    "feature_df.dropna(how='any', axis=0, inplace=True)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our label determined, let's prepare the data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(feature_df['genre_top'].unique(), range(0,10)))\n",
    "y = feature_df['genre_top'].map(label_mapping, na_action='ignore').values\n",
    "X = feature_df.drop(columns=['genre_top'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Accuracy\n",
    "For our accuracy score, we are using F-Measure. This accuracy score is controlled by the actual positive values, the false positives, and the false negatives.  A high F-Measure represents low false positives and negatives. This accuracy score best fits our given data set. False positives and false negatives are equally problematic in our data set. If we mischaracterize too many songs’ genres as Genre A; suddenly, Genre A gets flooded with hundreds of songs Genre A fans do not want to listen to. The service is no longer able to provide users with music that is audibly similar. If Band A produces a lot of songs that are Genre A but classified as Genre B, then users who listen to Genre A are unable to find songs by Band A, which is bad for the artist. Users who listen to Genre B are flooded with mischaracterized songs they do not want to listen to. Again, the service is no longer able to provide users with music that is audibly similar. All of these cases make a music streaming service into an unreliable source of new music and would drive users away. Since both of these cases are equally bad, we will use an accuracy score that weighs itself by both cases equally.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "my_scorer = make_scorer(f1_score, average = 'micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Cross-Validation\n",
    "Before we determine how we are going to cross-validate the classes, we should get some more information of the class division of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numInY(Y):\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    count_3 = 0\n",
    "    count_4 = 0\n",
    "    count_5 = 0\n",
    "    count_6 = 0\n",
    "    for i in y:\n",
    "        if i == 0:\n",
    "            count_0 = count_0 + 1\n",
    "        if i == 1:\n",
    "            count_1 = count_1 + 1\n",
    "        if i == 2:\n",
    "            count_2 = count_2 + 1\n",
    "        if i == 3:\n",
    "            count_3 = count_3 + 1\n",
    "        if i == 4:\n",
    "            count_4 = count_4 + 1\n",
    "        if i == 5:\n",
    "            count_5 = count_5 + 1\n",
    "        if i == 6:\n",
    "            count_6 = count_6 + 1\n",
    "    return [count_0, count_1, count_2, count_3, count_4, count_5, count_6] \n",
    "            \n",
    "plt.pie(x=numInY(y),autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a massive class imbalance, where two classes alone make up over 50% of the data itself. In order to maintain even class division, any form of cross validation we choose will have to stratify the data. Otherwise we may get entire sets of data that only contain one class. This could skew our model’s training to just recognize everything as a single class, as in a fold of the data with only one class, this would be the most correct assumption. We also have another issue; since, the individual songs are grouped together by artist, we could get folds that have a dispositional amount of songs by the same artist in one genre. Band A and Band B may both be Genre C but have different sounding songs. We do not want a model only trained to recognize Band A as Genre C. In order to combat this, we must also shuffle the data in our cross validation. For these reasons, we are using a stratified shuffle split. This is an accurate reflection of real world test cases, where both a variety of songs from different artist and different genres will be tested against the model. Music streaming services are constantly getting new songs from new artist from a wide variety of different genres. However, some genres are more popular than others and therefore will have more songs belonging to them. This is bias towards genres is reflected in the stratification of the model’s folds. Musicians across the globe are able to produce and submit songs to music streaming services. These musicians while working in the same genre will not have same influences and could produce songs of the same genre that do not exactly like one another. To reflect the difference is artist, we shuffle the data, ensuring no fold is too focused on one specific artist’s definition of a genre. \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "cv = StratifiedShuffleSplit(n_splits=10, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronBase(object):\n",
    "    def __init__(self, layers=2, phi='linear', cost_fn='quadratic', n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.layers=layers\n",
    "        self.phi = phi\n",
    "        self.cost_fn = cost_fn\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        weights = []\n",
    "        W1_num_elems = (self.n_features_ + 1) * self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1)  # reshape to be W\n",
    "        weights.append(W1)\n",
    "        for i in range(1, self.layers-1):\n",
    "            W_num_elems = ((weights[i-1].shape[0]+1)*(self.n_hidden))\n",
    "            W = np.random.uniform(-1.0, 1.0, size=W_num_elems)\n",
    "            W = W.reshape(self.n_hidden, weights[i-1].shape[0]+1)\n",
    "            weights.append(W)\n",
    "        Wfinal_num_elems = (self.n_hidden + 1) * self.n_output_\n",
    "        Wfinal = np.random.uniform(-1.0, 1.0, size=Wfinal_num_elems)\n",
    "        Wfinal = Wfinal.reshape(self.n_output_, self.n_hidden + 1)  # reshape to be W\n",
    "        weights.append(Wfinal)\n",
    "        return weights\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(phi, z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        if phi == 'sigmoid':\n",
    "            return expit(z)\n",
    "        elif phi == 'relu':\n",
    "            return np.maximum(z, 0, z)\n",
    "        elif phi == 'silu':\n",
    "            return z * expit(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, weights):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_ / 2.0) * np.sqrt(sum(np.mean(weight[:, 1:] ** 2) for weight in weights))\n",
    "\n",
    "    def _cost(self, A, Y_enc, weights):\n",
    "        '''Get the objective function value'''\n",
    "        if self.cost_fn == 'cross_entropy':\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc * np.log(A) + (1 - Y_enc) * np.log(1 - A))))\n",
    "        else:\n",
    "            cost = np.mean((Y_enc - A) ** 2)\n",
    "        L2_term = self._L2_reg(self.l2_C, weights)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, weights):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A = []\n",
    "        Z = []\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = weights[0] @ A1\n",
    "        A.append(A1)\n",
    "        Z.append(Z1)\n",
    "        for i in range(1, len(weights)):\n",
    "            A_new = self._sigmoid(self.phi, Z1)\n",
    "            A_new = self._add_bias_unit(A_new, how='row')\n",
    "            Z_new = weights[i] @ A_new\n",
    "            A.append(A_new)\n",
    "            Z.append(Z_new)\n",
    "        A_final = self._sigmoid(self.phi, Z[-1])\n",
    "        A.append(A_final)\n",
    "        return A, Z\n",
    "\n",
    "    def _get_gradient(self, A, Z, Y_enc, weights):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        gradients = []\n",
    "        V = []\n",
    "\n",
    "        if self.cost_fn == 'cross_entropy':\n",
    "            Vfinal = (A[-1] - Y_enc)\n",
    "        else:\n",
    "            Vfinal = -2 * (Y_enc - A[-1]) * A[-1] * (1 - A[-1])  # last layer sensitivity\n",
    "        V.append(Vfinal)\n",
    "\n",
    "        gradfinal = Vfinal @ A[-2].T  # no bias on final layer\n",
    "        #gradfinal = gradfinal.clip(-.25, .25)\n",
    "        gradients.append(np.clip(gradfinal, -.25, .25))\n",
    "\n",
    "        Vsecond_to_last = A[-2] * (1 - A[-2]) * (weights[-1].T @ V[0])  # back prop the sensitivity\n",
    "        V.append(Vsecond_to_last)\n",
    "        gradsecond_to_last = V[-1][1:, :] @ A[-3].T  # dont back prop sensitivity of bias\n",
    "\n",
    "        gradients.append(np.clip(gradsecond_to_last, -.25, .25))\n",
    "        for i in range(2, len(A)-1):\n",
    "            Vcurr =  A[-i-1] * (1 - A[-i-1]) * (weights[-i].T @ V[i-1][1:, :])  # back prop the sensitivity\n",
    "            V.append(Vcurr)\n",
    "            gradcurr = Vcurr[1:, :] @ A[-i-2].T\n",
    "            gradcurr = gradcurr.clip(-.25, .25)\n",
    "            gradients.append(gradcurr)\n",
    "        gradients.reverse()\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        gradients[0][:, 1:] += weights[0][:, 1:] * self.l2_C\n",
    "        for weight_index in range(1, len(weights)-1):\n",
    "            gradients[weight_index][:, 1:] += weights[weight_index][:, 1:] * self.l2_C\n",
    "        gradients[-1][:, 1:] += weights[-1][:, 1:] * self.l2_C\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        A, Z = self._feedforward(X, self.weights)\n",
    "        y_pred = np.argmax(A[-1], axis=0)\n",
    "        return y_pred\n",
    "\n",
    "# just start with the vectorized version and minibatch\n",
    "class MLP(MultiLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.weights = self._initialize_weights()\n",
    "\n",
    "        delta_weights_prev =  [np.zeros(weight.shape) for weight in self.weights]\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(f1_score(y_data, self.predict(X_data), average='micro'))\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const * i)\n",
    "\n",
    "            if print_progress > 0 and (i + 1) % print_progress == 0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i + 1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "                # feedforward\n",
    "                A, Z = self._feedforward(X_data[idx],self.weights)\n",
    "\n",
    "                cost = self._cost(A[-1], Y_enc[:, idx], self.weights)\n",
    "                mini_cost.append(cost)  # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradients = self._get_gradient(A=A, Z=Z, Y_enc=Y_enc[:, idx],\n",
    "                                                  weights=self.weights)\n",
    "\n",
    "                # momentum calculations\n",
    "                delta_weights = [self.eta * grad for grad in gradients]\n",
    "                for weight_index in range(0, len(self.weights)):\n",
    "                    self.weights[weight_index] -= (delta_weights[weight_index] + (self.alpha * delta_weights_prev[weight_index]))\n",
    "                delta_weights_prev = delta_weights\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(f1_score(y_data, self.predict(X_data), average='micro'))\n",
    "\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
